import os
import io
import time
import base64
import tempfile
from typing import List, Dict, Any, Tuple
from datetime import datetime 
import streamlit as st
import yaml
import plotly.express as px
import plotly.graph_objects as go
from PIL import Image
import pandas as pd 
# Embedded modules (combined)
import pdfplumber
from pdf2image import convert_from_bytes
import pytesseract
from openai import OpenAI
import google.generativeai as genai
from xai_sdk import Client as XAIClient
from xai_sdk.chat import user as xai_user, system as xai_system 
# ==================== THEME SYSTEM ====================
FLOWER_THEMES = { 
    "æ«»èŠ± Cherry Blossom": { 
        "primary": "#FFB7C5", 
        "secondary": "#FFC0CB", 
        "accent": "#FF69B4", 
        "bg_light": "linear-gradient(135deg, #ffe6f0 0%, #fff5f8 50%, #ffe6f0 100%)", 
        "bg_dark": "linear-gradient(135deg, #2d1b2e 0%, #3d2533 50%, #2d1b2e 100%)", 
        "icon": "ğŸŒ¸" 
    }, 
    "ç«ç‘° Rose": { 
        "primary": "#E91E63", 
        "secondary": "#F06292", 
        "accent": "#C2185B", 
        "bg_light": "linear-gradient(135deg, #fce4ec 0%, #fff 50%, #fce4ec 100%)", 
        "bg_dark": "linear-gradient(135deg, #1a0e13 0%, #2d1420 50%, #1a0e13 100%)", 
        "icon": "ğŸŒ¹" 
    }, 
    "è–°è¡£è‰ Lavender": { 
        "primary": "#9C27B0", 
        "secondary": "#BA68C8", 
        "accent": "#7B1FA2", 
        "bg_light": "linear-gradient(135deg, #f3e5f5 0%, #fff 50%, #f3e5f5 100%)", 
        "bg_dark": "linear-gradient(135deg, #1a0d1f 0%, #2d1a33 50%, #1a0d1f 100%)", 
        "icon": "ğŸ’œ" 
    }, 
    "é¬±é‡‘é¦™ Tulip": { 
        "primary": "#FF5722", 
        "secondary": "#FF8A65", 
        "accent": "#E64A19", 
        "bg_light": "linear-gradient(135deg, #fbe9e7 0%, #fff 50%, #fbe9e7 100%)", 
        "bg_dark": "linear-gradient(135deg, #1f0e0a 0%, #331814 50%, #1f0e0a 100%)", 
        "icon": "ğŸŒ·" 
    }, 
    "å‘æ—¥è‘µ Sunflower": { 
        "primary": "#FFC107", 
        "secondary": "#FFD54F", 
        "accent": "#FFA000", 
        "bg_light": "linear-gradient(135deg, #fff9e6 0%, #fffef5 50%, #fff9e6 100%)", 
        "bg_dark": "linear-gradient(135deg, #1f1a0a 0%, #332814 50%, #1f1a0a 100%)", 
        "icon": "ğŸŒ»" 
    }, 
    "è“®èŠ± Lotus": { 
        "primary": "#E91E8C", 
        "secondary": "#F48FB1", 
        "accent": "#AD1457", 
        "bg_light": "linear-gradient(135deg, #fce4f0 0%, #fff 50%, #fce4f0 100%)", 
        "bg_dark": "linear-gradient(135deg, #1f0e1a 0%, #331826 50%, #1f0e1a 100%)", 
        "icon": "ğŸª·" 
    }, 
    "è˜­èŠ± Orchid": { 
        "primary": "#9C27B0", 
        "secondary": "#CE93D8", 
        "accent": "#6A1B9A", 
        "bg_light": "linear-gradient(135deg, #f3e5f5 0%, #faf5ff 50%, #f3e5f5 100%)", 
        "bg_dark": "linear-gradient(135deg, #1a0d1f 0%, #2d1a33 50%, #1a0d1f 100%)", 
        "icon": "ğŸŒº" 
    }, 
    "èŒ‰è‰ Jasmine": { 
        "primary": "#4CAF50", 
        "secondary": "#81C784", 
        "accent": "#388E3C", 
        "bg_light": "linear-gradient(135deg, #e8f5e9 0%, #f1f8f1 50%, #e8f5e9 100%)", 
        "bg_dark": "linear-gradient(135deg, #0a1f0d 0%, #14331a 50%, #0a1f0d 100%)", 
        "icon": "ğŸ¤" 
    }, 
    "ç‰¡ä¸¹ Peony": { 
        "primary": "#E91E63", 
        "secondary": "#F06292", 
        "accent": "#C2185B", 
        "bg_light": "linear-gradient(135deg, #fce4ec 0%, #fff 50%, #fce4ec 100%)", 
        "bg_dark": "linear-gradient(135deg, #1f0e13 0%, #331826 50%, #1f0e13 100%)", 
        "icon": "ğŸŒº" 
    }, 
    "ç™¾åˆ Lily": { 
        "primary": "#FFFFFF", 
        "secondary": "#F5F5F5", 
        "accent": "#E0E0E0", 
        "bg_light": "linear-gradient(135deg, #fafafa 0%, #fff 50%, #fafafa 100%)", 
        "bg_dark": "linear-gradient(135deg, #0d0d0d 0%, #1a1a1a 50%, #0d0d0d 100%)", 
        "icon": "âšª" 
    }, 
    "ç´«ç¾…è˜­ Violet": { 
        "primary": "#673AB7", 
        "secondary": "#9575CD", 
        "accent": "#512DA8", 
        "bg_light": "linear-gradient(135deg, #ede7f6 0%, #f8f5ff 50%, #ede7f6 100%)", 
        "bg_dark": "linear-gradient(135deg, #0d0a1f 0%, #1a1433 50%, #0d0a1f 100%)", 
        "icon": "ğŸ’œ" 
    }, 
    "æ¢…èŠ± Plum Blossom": { 
        "primary": "#E91E63", 
        "secondary": "#F48FB1", 
        "accent": "#C2185B", 
        "bg_light": "linear-gradient(135deg, #fce4ec 0%, #fff5f8 50%, #fce4ec 100%)", 
        "bg_dark": "linear-gradient(135deg, #1f0e13 0%, #2d1a20 50%, #1f0e13 100%)", 
        "icon": "ğŸŒ¸" 
    }, 
    "èŒ¶èŠ± Camellia": { 
        "primary": "#D32F2F", 
        "secondary": "#EF5350", 
        "accent": "#B71C1C", 
        "bg_light": "linear-gradient(135deg, #ffebee 0%, #fff 50%, #ffebee 100%)", 
        "bg_dark": "linear-gradient(135deg, #1f0a0a 0%, #330d0d 50%, #1f0a0a 100%)", 
        "icon": "ğŸŒ¹" 
    }, 
    "åº·ä¹ƒé¦¨ Carnation": { 
        "primary": "#F06292", 
        "secondary": "#F8BBD0", 
        "accent": "#E91E63", 
        "bg_light": "linear-gradient(135deg, #fce4ec 0%, #fff5f8 50%, #fce4ec 100%)", 
        "bg_dark": "linear-gradient(135deg, #1f0e13 0%, #2d1a20 50%, #1f0e13 100%)", 
        "icon": "ğŸ’" 
    }, 
    "æµ·æ£  Begonia": { 
        "primary": "#FF5252", 
        "secondary": "#FF8A80", 
        "accent": "#D50000", 
        "bg_light": "linear-gradient(135deg, #ffebee 0%, #fff 50%, #ffebee 100%)", 
        "bg_dark": "linear-gradient(135deg, #1f0a0a 0%, #330d0d 50%, #1f0a0a 100%)", 
        "icon": "ğŸŒº" 
    }, 
    "æ¡‚èŠ± Osmanthus": { 
        "primary": "#FF9800", 
        "secondary": "#FFB74D", 
        "accent": "#F57C00", 
        "bg_light": "linear-gradient(135deg, #fff3e0 0%, #fffaf5 50%, #fff3e0 100%)", 
        "bg_dark": "linear-gradient(135deg, #1f140a 0%, #332014 50%, #1f140a 100%)", 
        "icon": "ğŸŸ¡" 
    }, 
    "ç´«è—¤ Wisteria": { 
        "primary": "#9C27B0", 
        "secondary": "#BA68C8", 
        "accent": "#7B1FA2", 
        "bg_light": "linear-gradient(135deg, #f3e5f5 0%, #faf5ff 50%, #f3e5f5 100%)", 
        "bg_dark": "linear-gradient(135deg, #1a0d1f 0%, #2d1a33 50%, #1a0d1f 100%)", 
        "icon": "ğŸ’œ" 
    }, 
    "æ°´ä»™ Narcissus": { 
        "primary": "#FFEB3B", 
        "secondary": "#FFF59D", 
        "accent": "#F9A825", 
        "bg_light": "linear-gradient(135deg, #fffde7 0%, #fffff5 50%, #fffde7 100%)", 
        "bg_dark": "linear-gradient(135deg, #1f1f0a 0%, #33330d 50%, #1f1f0a 100%)", 
        "icon": "ğŸŒ¼" 
    }, 
    "æœéµ‘ Azalea": { 
        "primary": "#E91E63", 
        "secondary": "#F06292", 
        "accent": "#C2185B", 
        "bg_light": "linear-gradient(135deg, #fce4ec 0%, #fff 50%, #fce4ec 100%)", 
        "bg_dark": "linear-gradient(135deg, #1f0e13 0%, #2d1a20 50%, #1f0e13 100%)", 
        "icon": "ğŸŒ¸" 
    }, 
    "èŠ™è“‰ Hibiscus": { 
        "primary": "#FF5722", 
        "secondary": "#FF8A65", 
        "accent": "#E64A19", 
        "bg_light": "linear-gradient(135deg, #fbe9e7 0%, #fff 50%, #fbe9e7 100%)", 
        "bg_dark": "linear-gradient(135deg, #1f0e0a 0%, #331814 50%, #1f0e0a 100%)", 
        "icon": "ğŸŒº" 
    }
}
TRANSLATIONS = { 
    "zh_TW": { 
        "title": "ğŸŒ¸ TFDA Agentic AIä»£ç†äººè¼”åŠ©å¯©æŸ¥ç³»çµ±", 
        "subtitle": "æ™ºæ…§æ–‡ä»¶åˆ†æèˆ‡è³‡æ–™æå– AI ä»£ç†äººå¹³å°", 
        "theme_selector": "é¸æ“‡èŠ±å‰ä¸»é¡Œ", 
        "language": "èªè¨€", 
        "dark_mode": "æ·±è‰²æ¨¡å¼", 
        "upload_tab": "1) ä¸Šå‚³èˆ‡OCR", 
        "preview_tab": "2) é è¦½èˆ‡ç·¨è¼¯", 
        "config_tab": "3) ä»£ç†è¨­å®š", 
        "execute_tab": "4) åŸ·è¡Œ", 
        "dashboard_tab": "5) å„€è¡¨æ¿", 
        "notes_tab": "6) å¯©æŸ¥ç­†è¨˜",
        "advanced_tab": "7) é€²éšæ¯”è¼ƒ",
        "upload_pdf": "ä¸Šå‚³ PDF æª”æ¡ˆ", 
        "ocr_mode": "OCR æ¨¡å¼", 
        "ocr_lang": "OCR èªè¨€", 
        "page_range": "é ç¢¼ç¯„åœ", 
        "start_ocr": "é–‹å§‹ OCR", 
        "save_agents": "å„²å­˜ agents.yaml", 
        "download_agents": "ä¸‹è¼‰ agents.yaml", 
        "reset_agents": "é‡ç½®ç‚ºé è¨­", 
        "providers": "API ä¾›æ‡‰å•†", 
        "connected": "å·²é€£ç·š", 
        "not_connected": "æœªé€£ç·š" 
    }, 
    "en": { 
        "title": "ğŸŒ¸ TFDA Agentic AI Assistance Review System", 
        "subtitle": "Intelligent Document Analysis & Data Extraction AI Agent Platform", 
        "theme_selector": "Select Floral Theme", 
        "language": "Language", 
        "dark_mode": "Dark Mode", 
        "upload_tab": "1) Upload & OCR", 
        "preview_tab": "2) Preview & Edit", 
        "config_tab": "3) Agent Config", 
        "execute_tab": "4) Execute", 
        "dashboard_tab": "5) Dashboard", 
        "notes_tab": "6) Review Notes",
        "advanced_tab": "7) Adcanced agent",
        "upload_pdf": "Upload PDF File", 
        "ocr_mode": "OCR Mode", 
        "ocr_lang": "OCR Language", 
        "page_range": "Page Range", 
        "start_ocr": "Start OCR", 
        "save_agents": "Save agents.yaml", 
        "download_agents": "Download agents.yaml", 
        "reset_agents": "Reset to Default", 
        "providers": "API Providers", 
        "connected": "Connected", 
        "not_connected": "Not Connected" 
    }
} 
# ==================== LLM ROUTER ====================
ModelChoice = { 
    "gpt-5-nano": "openai", 
    "gpt-4o-mini": "openai", 
    "gpt-4.1-mini": "openai", 
    "gemini-2.5-flash": "gemini", 
    "gemini-2.5-flash-lite": "gemini", 
    "grok-4-fast-reasoning": "grok", 
    "grok-3-mini": "grok",
}
class LLMRouter: 
    def __init__(self): 
        self._openai_client = None 
        self._gemini_ready = False 
        self._xai_client = None 
        self._init_clients() 
    def _init_clients(self): 
        if os.getenv("OPENAI_API_KEY"): 
            self._openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY")) 
        if os.getenv("GEMINI_API_KEY"): 
            genai.configure(api_key=os.getenv("GEMINI_API_KEY")) 
            self._gemini_ready = True 
        if os.getenv("XAI_API_KEY"): 
            self._xai_client = XAIClient(api_key=os.getenv("XAI_API_KEY"), timeout=3600) 
    def generate_text(self, model_name: str, messages: List[Dict], params: Dict) -> Tuple[str, Dict, str]: 
        provider = ModelChoice.get(model_name, "openai") 
        if provider == "openai": 
            return self._openai_chat(model_name, messages, params), {"total_tokens": self._estimate_tokens(messages)}, "OpenAI" 
        elif provider == "gemini": 
            return self._gemini_chat(model_name, messages, params), {"total_tokens": self._estimate_tokens(messages)}, "Gemini" 
        elif provider == "grok": 
            return self._grok_chat(model_name, messages, params), {"total_tokens": self._estimate_tokens(messages)}, "Grok" 
    def generate_vision(self, model_name: str, prompt: str, images: List) -> str: 
        provider = ModelChoice.get(model_name, "openai") 
        if provider == "gemini": 
            return self._gemini_vision(model_name, prompt, images) 
        elif provider == "openai": 
            return self._openai_vision(model_name, prompt, images) 
        return "Vision not supported" 
    def _openai_chat(self, model: str, messages: List, params: Dict) -> str: 
        resp = self._openai_client.chat.completions.create( 
            model=model, 
            messages=messages, 
            temperature=params.get("temperature", 0.4), 
            top_p=params.get("top_p", 0.95), 
            max_tokens=params.get("max_tokens", 800) 
        ) 
        return resp.choices[0].message.content 
    def _gemini_chat(self, model: str, messages: List, params: Dict) -> str: 
        mm = genai.GenerativeModel(model) 
        sys = "\n".join([m["content"] for m in messages if m["role"] == "system"]).strip() 
        usr = "\n".join([m["content"] for m in messages if m["role"] == "user"]).strip() 
        final = (sys + "\n\n" + usr).strip() if sys else usr 
        resp = mm.generate_content(final, generation_config=genai.types.GenerationConfig( 
            temperature=params.get("temperature", 0.4), 
            top_p=params.get("top_p", 0.95), 
            max_output_tokens=params.get("max_tokens", 800) 
        )) 
        return resp.text 
    def _grok_chat(self, model: str, messages: List, params: Dict) -> str: 
        chat = self._xai_client.chat.create(model=model) 
        for m in messages: 
            if m["role"] == "system": 
                chat.append(xai_system(m["content"])) 
            elif m["role"] == "user": 
                chat.append(xai_user(m["content"])) 
        return chat.sample().content 
    def _gemini_vision(self, model: str, prompt: str, images: List) -> str: 
        mm = genai.GenerativeModel(model) 
        parts = [prompt] + [genai.Image.from_pil(img) for img in images] 
        return mm.generate_content(parts).text 
    def _openai_vision(self, model: str, prompt: str, images: List) -> str: 
        contents = [{"type": "text", "text": prompt}] 
        for img in images: 
            buf = io.BytesIO() 
            img.save(buf, format="PNG") 
            b64 = base64.b64encode(buf.getvalue()).decode("utf-8") 
            contents.append({"type": "image_url", "image_url": {"url": f"data:image/png;base64,{b64}"}}) 
        resp = self._openai_client.chat.completions.create( 
            model=model, 
            messages=[{"role": "user", "content": contents}] 
        ) 
        return resp.choices[0].message.content 
    def _estimate_tokens(self, messages: List) -> int: 
        return max(1, sum(len(m.get("content", "")) for m in messages) // 4) 
# ==================== OCR FUNCTIONS ====================
def render_pdf_pages(pdf_bytes: bytes, dpi: int = 150, max_pages: int = 30) -> List[Tuple[int, Image.Image]]: 
    pages = convert_from_bytes(pdf_bytes, dpi=dpi, first_page=1, last_page=None) 
    return [(idx, im) for idx, im in enumerate(pages[:max_pages])] 
def extract_text_python(pdf_bytes: bytes, selected_pages: List[int], ocr_language: str = "english") -> str: 
    text_parts = [] 
    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf: 
        for i in selected_pages: 
            if i < len(pdf.pages): 
                txt = pdf.pages[i].extract_text() or "" 
                if txt.strip(): 
                    text_parts.append(f"[PAGE {i+1} - TEXT]\n{txt.strip()}\n") 
    lang = "eng" if ocr_language == "english" else "chi_tra" 
    for p in selected_pages: 
        ims = convert_from_bytes(pdf_bytes, dpi=220, first_page=p+1, last_page=p+1) 
        if ims: 
            t = pytesseract.image_to_string(ims[0], lang=lang) 
            if t.strip(): 
                text_parts.append(f"[PAGE {p+1} - OCR]\n{t.strip()}\n") 
    return "\n".join(text_parts).strip() 
def extract_text_llm(page_images: List[Image.Image], model_name: str, router) -> str: 
    prompt = "è«‹å°‡åœ–ç‰‡ä¸­çš„æ–‡å­—å®Œæ•´è½‰éŒ„ï¼ˆä¿æŒåŸæ–‡ã€æ®µè½èˆ‡æ¨™é»ï¼‰ã€‚è‹¥æœ‰è¡¨æ ¼ï¼Œè«‹ä»¥Markdownè¡¨æ ¼å‘ˆç¾ã€‚" 
    text_blocks = [] 
    for idx, im in enumerate(page_images): 
        out = router.generate_vision(model_name, f"{prompt}\né é¢ {idx+1}ï¼š", [im]) 
        text_blocks.append(f"[PAGE {idx+1} - LLM OCR]\n{out}\n") 
    return "\n".join(text_blocks).strip() 
# ==================== APP CONFIG ====================
st.set_page_config( 
    page_title="ğŸŒ¸ TFDA Agentic AI Assistance Review System", 
    page_icon="ğŸŒ¸", 
    layout="wide", 
    initial_sidebar_state="expanded"
) 
# ==================== SESSION STATE ====================
if "theme" not in st.session_state: 
    st.session_state.theme = "æ«»èŠ± Cherry Blossom"
if "dark_mode" not in st.session_state: 
    st.session_state.dark_mode = False
if "language" not in st.session_state: 
    st.session_state.language = "zh_TW"
if "agents_config" not in st.session_state: 
    st.session_state.agents_config = []
if "ocr_text" not in st.session_state: 
    st.session_state.ocr_text = ""
if "page_images" not in st.session_state: 
    st.session_state.page_images = []
if "agent_outputs" not in st.session_state: 
    st.session_state.agent_outputs = []
if "selected_agent_count" not in st.session_state: 
    st.session_state.selected_agent_count = 5
if "run_metrics" not in st.session_state: 
    st.session_state.run_metrics = []
if "review_notes" not in st.session_state: 
    st.session_state.review_notes = "# å¯©æŸ¥ç­†è¨˜\n\nåœ¨é€™è£¡è¨˜éŒ„æ‚¨çš„å¯©æŸ¥ç­†è¨˜ã€‚æ”¯æ´ Markdown æ ¼å¼ã€‚\n\nä½¿ç”¨ HTML æ¨™ç±¤æ”¹è®Šæ–‡å­—é¡è‰²ï¼Œä¾‹å¦‚ï¼š<span style='color:red'>ç´…è‰²æ–‡å­—</span>\n\n## å¾ŒçºŒå•é¡Œ\n- å•é¡Œ1ï¼Ÿ\n- å•é¡Œ2ï¼Ÿ"
# ==================== DEFAULT FDA AGENTS ====================
DEFAULT_FDA_AGENTS = """agents: 
  - name: ç”³è«‹è³‡æ–™é‡é»åˆ†æèˆ‡æ‘˜è¦å°ˆå®¶ 
    description: ç”³è«‹è³‡æ–™é‡é»åˆ†æèˆ‡æ‘˜è¦å°ˆå®¶
    system_prompt: | 
      ä½ æ˜¯ä¸€ä½é†«ç™‚å™¨ææ³•è¦å°ˆå®¶ã€‚æ ¹æ“šæä¾›çš„æ–‡ä»¶ï¼Œé€²è¡Œç¹é«”ä¸­æ–‡æ‘˜è¦in markdown in traditional chinese with keywords in coral color. Please also create a table include 20 key itemsã€‚
      - è­˜åˆ¥ï¼šå» å•†åç¨±ã€åœ°å€ã€å“åã€é¡åˆ¥ã€è­‰æ›¸ç·¨è™Ÿã€æ—¥æœŸã€æ©Ÿæ§‹ 
      - æ¨™è¨»ä¸ç¢ºå®šé …ç›®ï¼Œä¿ç•™åŸæ–‡å¼•ç”¨ 
      - ä»¥çµæ§‹åŒ–æ ¼å¼è¼¸å‡ºï¼ˆè¡¨æ ¼æˆ–JSONï¼‰ 
    user_prompt: "ä½ æ˜¯ä¸€ä½é†«ç™‚å™¨ææ³•è¦å°ˆå®¶ã€‚æ ¹æ“šæä¾›çš„æ–‡ä»¶ï¼Œé€²è¡Œç¹é«”ä¸­æ–‡æ‘˜è¦in markdown in traditional chinese with keywords in coral color. Please also create a table include 20 key itemsã€‚" 
    model: gpt-4o-mini 
    temperature: 0 
    top_p: 0.9 
    max_tokens: 3000 
  - name: åˆç´„è³‡æ–™åˆ†æå¸« 
    description: åˆç´„è³‡æ–™åˆ†æå¸«
    system_prompt: | 
      åˆç´„è³‡æ–™åˆ†æå¸«ï¼Œè«‹ç¢ºèªåˆç´„ä¸­åŒ…å«ä»¥ä¸‹å…§å®¹ï¼Œè«‹æ‘˜è¦åˆç´„å…§å®¹ã€‚ 
      - å§”è¨—è€…åŠå—è¨—è€…ä¹‹åç¨±åŠåœ°å€ï¼š å§”è¨—è€…(ç”²æ–¹)åç¨±ã€åœ°å€ï¼Œå—è¨—è€…(ä¹™æ–¹)åç¨±ã€åœ°å€
      - è¨—è£½é€ ä¹‹åˆæ„ï¼šå§”è¨—è€…ç¾©å‹™ã€å—è¨—è€…ç¾©å‹™ã€‚ 
      - å§”è¨—è£½é€ ä¹‹é†«ç™‚å™¨æåˆ†é¡åˆ†ç´šå“é …ï¼šå“é …åç¨±ï¼š(èˆ‰ä¾‹ M.5925 è»Ÿå¼éš±å½¢çœ¼é¡)ã€ç®¡ç†ç­‰ç´šï¼š(èˆ‰ä¾‹ç¬¬äºŒç­‰ç´š) 
      - å§”è¨—è£½é€ ä¹‹è£½ç¨‹ï¼šå§”è¨—è£½ç¨‹ç¯„åœï¼š(èˆ‰ä¾‹ï¼šå…¨éƒ¨è£½ç¨‹å§”è¨—è£½é€ ã€æ»…èŒã€åŸæ–™æº–å‚™ã€æ¨¡å…·æˆå‹ã€é¡ç‰‡åŠ å·¥ã€åŒ…è£ã€å“è³ªæª¢é©—ç­‰å…¨éƒ¨è£½ç¨‹ã€‚ 
      - å§”è¨—è€…åŠå—è¨—è€…ä¹‹æ¬Šåˆ©ç¾©å‹™ï¼šå§”è¨—è€…æ¬Šåˆ©ç¾©å‹™ï¼šèˆ‰ä¾‹ï¼šæœ‰æ¬ŠæŸ¥æ ¸è£½é€ ç´€éŒ„åŠå“è³ªç®¡ç†æ–‡ä»¶ã€‚æ‡‰æä¾›å¿…è¦ä¹‹æŠ€è¡“æ–‡ä»¶(MDF/DMR)åŠç”¢å“è¦æ ¼ã€‚æ‡‰ä¾ç´„å®šæ”¯ä»˜è£½é€ è²»ç”¨ã€‚ä¹™æ–¹æ‰€æœ‰ç”Ÿç”¢è£½ç¨‹æ‡‰ç¬¦åˆé†«ç™‚å™¨æå“è³ªç®¡ç†ç³»çµ±æº–å‰‡(QMS)åŠç›¸é—œæ³•ä»¤è¦æ±‚ã€‚ 
    user_prompt: "è«‹ç¢ºèªåˆç´„ä¸­åŒ…å«ä»¥ä¸‹å…§å®¹ï¼Œè«‹æ‘˜è¦åˆç´„å…§å®¹ in markdown in traditional chinese with keywords in coral color" 
    model: gpt-4o-mini 
    temperature: 0 
    top_p: 0.9 
    max_tokens: 3200 
  - name: é†«ç™‚å™¨æå§”è¨—è£½é€ åˆç´„å¯©æŸ¥å°ˆå®¶
    description: é†«ç™‚å™¨æå§”è¨—è£½é€ åˆç´„å¯©æŸ¥å°ˆå®¶ 
    system_prompt: | 
      é†«ç™‚å™¨æåˆç´„å¯©æŸ¥å°ˆå®¶ï¼Œè«‹ç¢ºèªåˆç´„è³‡æ–™æ˜¯å¦åŒ…å«ä»¥ä¸‹å¯©æŸ¥é‡é»å…§å®¹ï¼Œä¸¦æä¾›ç¶œåˆå¯©æŸ¥å»ºè­°ã€‚è‹¥ç›®å‰æä¾›çš„è³‡æ–™ä¸è¶³ä»¥åˆ¤å®šæ˜¯å¦ç¬¦åˆè¦å®šï¼Œè«‹å‘Šè¨´ä½¿ç”¨è€…æ‡‰è©²é€²ä¸€æ­¥æä¾›æˆ–ç¢ºèªé‚£äº›è³‡è¨Šã€‚ 
      - å§”è¨—è£½é€ åˆç´„æ‰€è¨˜è¼‰ä¹‹å§”è¨—è€…åŠå—è¨—è€…ä¹‹åç¨±åŠåœ°å€ï¼Œæ˜¯å¦èˆ‡ç”³è«‹æ›¸è¨˜è¼‰ä¹‹å§”è¨—è€…åŠå—è¨—è€…ä¹‹åç¨±åŠåœ°å€ä¸€è‡´ï¼š å§”è¨—è€…(ç”²æ–¹)åç¨±ã€åœ°å€ï¼Œå—è¨—è€…(ä¹™æ–¹)åç¨±ã€åœ°å€
      - è¨—è£½é€ ä¹‹åˆæ„ï¼šå§”è¨—è€…ç¾©å‹™ã€å—è¨—è€…ç¾©å‹™ã€‚ 
      - å§”è¨—è£½é€ ä¹‹é†«ç™‚å™¨æåˆ†é¡åˆ†ç´šå“é …ï¼Œæ˜¯å¦èˆ‡ç”³è«‹æ›¸è¨˜è¼‰ä¹‹å§”è¨—è£½é€ é†«ç™‚å™¨æåˆ†é¡åˆ†ç´šå“é …ä¸€è‡´ã€‚åˆ†é¡åˆ†ç´šå“é …(èˆ‰ä¾‹ M.5925 è»Ÿå¼éš±å½¢çœ¼é¡) 
      - å§”è¨—è£½é€ ä¹‹è£½ç¨‹ï¼Œæ˜¯å¦èˆ‡ç”³è«‹æ›¸è¨˜è¼‰ä¹‹å§”è¨—è£½ç¨‹ä¸€è‡´(èˆ‰ä¾‹ï¼šå§”è¨—è£½ç¨‹åŒ…å«å…¨éƒ¨è£½ç¨‹å§”è¨—è£½é€ (ç‰¹å®šåˆ†é¡åˆ†ç´šå“é …)ã€è£½é€ (ç‰¹å®šåˆ†é¡åˆ†ç´šå“é …)ã€æ»…èŒ(ç‰¹å®šæ»…èŒæ–¹å¼) 
      - å§”è¨—è€…åŠå—è¨—è€…ä¹‹æ¬Šåˆ©ç¾©å‹™ï¼šå§”è¨—è€…æ¬Šåˆ©ç¾©å‹™ï¼šèˆ‰ä¾‹ï¼šæœ‰æ¬ŠæŸ¥æ ¸è£½é€ ç´€éŒ„åŠå“è³ªç®¡ç†æ–‡ä»¶ã€‚æ‡‰æä¾›å¿…è¦ä¹‹æŠ€è¡“æ–‡ä»¶(MDF/DMR)åŠç”¢å“è¦æ ¼ã€‚æ‡‰ä¾ç´„å®šæ”¯ä»˜è£½é€ è²»ç”¨ã€‚ä¹™æ–¹æ‰€æœ‰ç”Ÿç”¢è£½ç¨‹æ‡‰ç¬¦åˆé†«ç™‚å™¨æå“è³ªç®¡ç†ç³»çµ±æº–å‰‡(QMS)åŠç›¸é—œæ³•ä»¤è¦æ±‚ã€‚ 
      - å§”è¨—è£½é€ åˆç´„æ˜¯å¦åŒ…å«å§”è¨—è€…èˆ‡å—è¨—è€…é›™æ–¹å…¬å¸ç”¨å°ï¼ŒåŠé›™æ–¹è² è²¬äººç°½åæˆ–ç”¨å°
    user_prompt: "è«‹ç¢ºèªåˆç´„è³‡æ–™æ˜¯å¦åŒ…å«ä»¥ä¸‹å¯©æŸ¥é‡é»å…§å®¹ï¼Œä¸¦æä¾›ç¶œåˆå¯©æŸ¥å»ºè­°ã€‚è‹¥ç›®å‰æä¾›çš„è³‡æ–™ä¸è¶³ä»¥åˆ¤å®šæ˜¯å¦ç¬¦åˆè¦å®šï¼Œè«‹å‘Šè¨´ä½¿ç”¨è€…æ‡‰è©²é€²ä¸€æ­¥æä¾›æˆ–ç¢ºèªé‚£äº›è³‡è¨Šã€‚" 
    model: gpt-4o-mini 
    temperature: 0.3 
    top_p: 0.9 
    max_tokens: 1500 
  - name: ä»¿å–®è®Šæ›´æ¯”å°å™¨ 
    description: æ¯”å°ä»¿å–®ç‰ˆæœ¬å·®ç•°ï¼Œè­˜åˆ¥é‡è¦è®Šæ›´ 
    system_prompt: | 
      ä½ æ˜¯æ³•è¦æ–‡ä»¶æ¯”å°å°ˆå®¶ã€‚ 
      - è­˜åˆ¥æ–°èˆŠç‰ˆæœ¬å·®ç•°ï¼ˆæ–°å¢ã€åˆªé™¤ã€ä¿®æ”¹ï¼‰ 
      - æ¨™è¨»é‡è¦å®‰å…¨æ€§è®Šæ›´ 
      - ä»¥å°ç…§è¡¨å‘ˆç¾å·®ç•° 
    user_prompt: "è«‹æ¯”å°ä»¥ä¸‹æ–‡ä»¶çš„ç‰ˆæœ¬å·®ç•°ï¼š" 
    model: gpt-4o-mini 
    temperature: 0.2 
    top_p: 0.9 
    max_tokens: 1200 
  - name: æ¨™ç±¤èˆ‡èªªæ˜æ›¸æª¢æŸ¥å™¨ 
    description: æª¢æŸ¥æ¨™ç±¤èªªæ˜æ›¸æ ¼å¼èˆ‡å®Œæ•´æ€§ 
    system_prompt: | 
      ä½ æ˜¯æ¨™ç¤ºå¯©æŸ¥å°ˆå®¶ã€‚ 
      - æª¢æŸ¥ï¼šå¿…è¦è³‡è¨Šå®Œæ•´æ€§ã€æ ¼å¼è¦ç¯„ 
      - è­˜åˆ¥ï¼šå­—é«”å¤§å°ã€è­¦èªæ¨™ç¤º 
      - æä¾›ä¿®æ”¹å»ºè­° 
    user_prompt: "è«‹æª¢æŸ¥ä»¥ä¸‹æ¨™ç±¤èªªæ˜æ›¸ï¼š" 
    model: gpt-4o-mini 
    temperature: 0.2 
    top_p: 0.9 
    max_tokens: 1000 
  - name: ç¶œåˆå ±å‘Šç”Ÿæˆå™¨ 
    description: æ•´åˆæ‰€æœ‰åˆ†æçµæœç”Ÿæˆå®Œæ•´å ±å‘Š 
    system_prompt: | 
      ä½ æ˜¯æ–‡ä»¶æ•´åˆå°ˆå®¶ã€‚ 
      - å½™æ•´ï¼šå‰è¿°æ‰€æœ‰ä»£ç†çš„åˆ†æçµæœ 
      - ç”Ÿæˆï¼šçµæ§‹åŒ–å®Œæ•´å ±å‘Š 
      - æ¨™è¨»ï¼šé‡é»ç™¼ç¾ã€é¢¨éšªè­¦ç¤ºã€å»ºè­°äº‹é … 
      - ä»¥å°ˆæ¥­æ ¼å¼è¼¸å‡ºï¼ˆå«ç›®éŒ„ã€ç« ç¯€ï¼‰ 
    user_prompt: "è«‹æ•´åˆä»¥ä¸‹æ‰€æœ‰åˆ†æçµæœç”Ÿæˆç¶œåˆå ±å‘Šï¼š" 
    model: gpt-4o-mini 
    temperature: 0.4 
    top_p: 0.95 
    max_tokens: 2000""" 
# ==================== LOAD/SAVE AGENTS ====================
def load_agents_yaml(yaml_text: str): 
    try: 
        data = yaml.safe_load(yaml_text) 
        st.session_state.agents_config = data.get("agents", []) 
        st.session_state.selected_agent_count = min(5, len(st.session_state.agents_config)) 
        st.session_state.agent_outputs = [ 
            {"input": "", "output": "", "time": 0.0, "tokens": 0, "provider": "", "model": ""} 
            for _ in st.session_state.agents_config 
        ] 
        return True 
    except Exception as e: 
        st.error(f"YAML è¼‰å…¥å¤±æ•—: {e}") 
        return False 
# ==================== THEME GENERATOR ====================
def generate_theme_css(theme_name: str, dark_mode: bool): 
    theme = FLOWER_THEMES[theme_name] 
    bg = theme["bg_dark"] if dark_mode else theme["bg_light"] 
    text_color = "#FFFFFF" if dark_mode else "#1a1a1a" 
    card_bg = "rgba(30, 30, 30, 0.85)" if dark_mode else "rgba(255, 255, 255, 0.85)" 
    border_color = theme["accent"] if dark_mode else theme["primary"] 
    return f""" 
    <style> 
        @import url('https://fonts.googleapis.com/css2?family=Noto+Sans+TC:wght@300;400;500;700&display=swap'); 
        [data-testid="stAppViewContainer"] > .main {{ 
            background: {bg}; 
            font-family: 'Noto Sans TC', sans-serif; 
            color: {text_color}; 
        }} 
        .block-container {{ 
            padding-top: 2rem; 
            padding-bottom: 3rem; 
            max-width: 1400px; 
        }} 
        .wow-card {{ 
            background: {card_bg}; 
            backdrop-filter: blur(15px); 
            border: 2px solid {border_color}40; 
            border-radius: 20px; 
            padding: 1.5rem; 
            margin: 1rem 0; 
            box-shadow: 0 8px 32px rgba(0,0,0,0.1); 
            transition: all 0.3s ease; 
        }} 
        .wow-card:hover {{ 
            transform: translateY(-2px); 
            box-shadow: 0 12px 48px rgba(0,0,0,0.15); 
            border-color: {border_color}80; 
        }} 
        .pill {{ 
            display: inline-flex; 
            align-items: center; 
            gap: 8px; 
            background: {theme['primary']}20; 
            color: {theme['accent']}; 
            border: 2px solid {theme['primary']}40; 
            padding: 8px 16px; 
            border-radius: 999px; 
            font-weight: 600; 
            font-size: 0.95rem; 
            transition: all 0.3s ease; 
        }} 
        .pill:hover {{ 
            background: {theme['primary']}40; 
            transform: scale(1.05); 
        }} 
        .badge-ok {{ 
            background: rgba(0, 200, 83, 0.15); 
            border-color: #00C85380; 
            color: #00C853; 
        }} 
        .badge-warn {{ 
            background: rgba(255, 193, 7, 0.15); 
            border-color: #FFC10780; 
            color: #F9A825; 
        }} 
        .badge-err {{ 
            background: rgba(244, 67, 54, 0.15); 
            border-color: #F4433680; 
            color: #D32F2F; 
        }} 
        .agent-step {{ 
            border-left: 6px solid {theme['accent']}; 
            background: {card_bg}; 
            border-radius: 16px; 
            padding: 1.5rem; 
            margin: 1rem 0; 
            box-shadow: 0 4px 16px rgba(0,0,0,0.08); 
        }} 
        h1, h2, h3 {{ 
            color: {theme['accent']} !important; 
            font-weight: 700; 
        }} 
        .stButton > button {{ 
            background: linear-gradient(135deg, {theme['primary']}, {theme['secondary']}); 
            color: white; 
            border: none; 
            border-radius: 12px; 
            padding: 0.75rem 2rem; 
            font-weight: 600; 
            transition: all 0.3s ease; 
            box-shadow: 0 4px 16px {theme['primary']}40; 
        }} 
        .stButton > button:hover {{ 
            transform: translateY(-2px); 
            box-shadow: 0 8px 24px {theme['primary']}60; 
        }} 
        .stTextInput > div > div > input, .stTextArea > div > div > textarea, .stSelectbox > div > div {{ 
            background: {card_bg}; 
            border: 2px solid {border_color}40; 
            border-radius: 12px; 
            color: {text_color}; 
        }} 
        .stTabs [data-baseweb="tab-list"] {{ 
            gap: 8px; 
            background: {card_bg}; 
            border-radius: 16px; 
            padding: 0.5rem; 
        }} 
        .stTabs [data-baseweb="tab"] {{ 
            border-radius: 12px; 
            color: {text_color}; 
            font-weight: 500; 
        }} 
        .stTabs [aria-selected="true"] {{ 
            background: linear-gradient(135deg, {theme['primary']}, {theme['secondary']}); 
            color: white; 
        }} 
        .metric-card {{ 
            background: {card_bg}; 
            border: 2px solid {theme['primary']}40; 
            border-radius: 16px; 
            padding: 1.5rem; 
            text-align: center; 
            transition: all 0.3s ease; 
        }} 
        .metric-card:hover {{ 
            transform: scale(1.05); 
            border-color: {theme['accent']}; 
        }} 
        .metric-value {{ 
            font-size: 2.5rem; 
            font-weight: 700; 
            color: {theme['accent']}; 
            margin: 0.5rem 0; 
        }} 
        .metric-label {{ 
            font-size: 0.9rem; 
            color: {text_color}80; 
            font-weight: 500; 
        }} 
    </style> 
    """ 
# ==================== INITIALIZE ====================
router = LLMRouter() 
# Load default agents if empty
if not st.session_state.agents_config: 
    load_agents_yaml(DEFAULT_FDA_AGENTS) 
# ==================== SIDEBAR ====================
with st.sidebar: 
    t = TRANSLATIONS[st.session_state.language] 
    st.markdown(f"### {t['theme_selector']}") 
    new_theme = st.selectbox( 
        "Theme", 
        list(FLOWER_THEMES.keys()), 
        index=list(FLOWER_THEMES.keys()).index(st.session_state.theme), 
        format_func=lambda x: f"{FLOWER_THEMES[x]['icon']} {x}", 
        label_visibility="collapsed" 
    ) 
    if new_theme != st.session_state.theme: 
        st.session_state.theme = new_theme 
        st.rerun() 
    col1, col2 = st.columns(2) 
    with col1: 
        new_dark = st.checkbox(t["dark_mode"], value=st.session_state.dark_mode) 
        if new_dark != st.session_state.dark_mode: 
            st.session_state.dark_mode = new_dark 
            st.rerun() 
    with col2: 
        new_lang = st.selectbox( 
            t["language"], 
            ["zh_TW", "en"], 
            index=0 if st.session_state.language == "zh_TW" else 1, 
            format_func=lambda x: "ç¹é«”ä¸­æ–‡" if x == "zh_TW" else "English" 
        ) 
        if new_lang != st.session_state.language: 
            st.session_state.language = new_lang 
            st.rerun() 
    st.markdown("---") 
    st.markdown(f"### ğŸ” {t['providers']}") 
    def show_provider_status(name: str, env_var: str): 
        connected = bool(os.getenv(env_var)) 
        status = t["connected"] if connected else t["not_connected"] 
        badge = "badge-ok" if connected else "badge-warn" 
        st.markdown(f'<div class="pill {badge}">{name}: {status}</div>', unsafe_allow_html=True) 
        if not connected: 
            key = st.text_input(f"{name} Key", type="password", key=f"key_{env_var}") 
            if key: 
                os.environ[env_var] = key 
                st.success(f"{name} {t['connected']}") 
    show_provider_status("OpenAI", "OPENAI_API_KEY") 
    show_provider_status("Gemini", "GEMINI_API_KEY") 
    show_provider_status("Grok", "XAI_API_KEY") 
    st.markdown("---") 
    st.markdown("### ğŸ¤– Agents YAML") 
    agents_text = st.text_area( 
        "agents.yaml", 
        value=yaml.dump({"agents": st.session_state.agents_config}, allow_unicode=True, sort_keys=False), 
        height=400, 
        label_visibility="collapsed" 
    ) 
    col_a, col_b, col_c = st.columns(3) 
    with col_a: 
        if st.button(t["save_agents"], use_container_width=True): 
            if load_agents_yaml(agents_text): 
                st.success("âœ… Saved!") 
    with col_b: 
        st.download_button( 
            t["download_agents"], 
            data=agents_text, 
            file_name=f"agents_{datetime.now().strftime('%Y%m%d_%H%M%S')}.yaml", 
            mime="text/yaml", 
            use_container_width=True 
        ) 
    with col_c: 
        if st.button(t["reset_agents"], use_container_width=True): 
            load_agents_yaml(DEFAULT_FDA_AGENTS) 
            st.success("âœ… Reset!") 
            st.rerun() 
# Apply theme
st.markdown(generate_theme_css(st.session_state.theme, st.session_state.dark_mode), unsafe_allow_html=True) 
# ==================== HEADER ====================
t = TRANSLATIONS[st.session_state.language] 
theme_icon = FLOWER_THEMES[st.session_state.theme]["icon"] 
col1, col2, col3 = st.columns([1, 3, 1])
with col1: 
    st.markdown(f'<div class="pill">{theme_icon} TFDA AI</div>', unsafe_allow_html=True)
with col2: 
    st.title(t["title"]) 
    st.caption(t["subtitle"])
with col3: 
    providers_ok = sum([ 
        bool(os.getenv("OPENAI_API_KEY")), 
        bool(os.getenv("GEMINI_API_KEY")), 
        bool(os.getenv("XAI_API_KEY")) 
    ]) 
    st.markdown(f""" 
        <div class="wow-card"> 
            <div class="metric-value">{providers_ok}/3</div> 
            <div class="metric-label">Active Providers</div> 
        </div> 
        """, unsafe_allow_html=True) 
st.markdown("---") 
# ==================== TABS ====================
tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs([
    t["upload_tab"], 
    t["preview_tab"], 
    t["config_tab"], 
    t["execute_tab"], 
    t["dashboard_tab"],
    t["notes_tab"],
    t["advanced_tab"]
])
# Tab 1: Upload & OCR
with tab1: 
    st.markdown('<div class="wow-card">', unsafe_allow_html=True) 
    st.subheader(f"{theme_icon} {t['upload_pdf']}") 
    uploaded = st.file_uploader(t["upload_pdf"], type=["pdf"], label_visibility="collapsed") 
    col1, col2, col3 = st.columns(3) 
    with col1: 
        ocr_mode = st.selectbox( 
            t["ocr_mode"], 
            ["Python OCR (pdfplumber + Tesseract)", "LLM OCR (Vision model)"] 
        ) 
    with col2: 
        ocr_lang = st.selectbox(t["ocr_lang"], ["english", "traditional-chinese"]) 
    with col3: 
        page_range_input = st.text_input(t["page_range"], value="1-5") 
    if ocr_mode.startswith("LLM"): 
        llm_ocr_model = st.selectbox("LLM Model", [ 
            "gemini-2.5-flash", 
            "gemini-2.5-flash-lite", 
            "gpt-4o-mini" 
        ]) 
    if uploaded: 
        pdf_bytes = uploaded.read() 
        with st.spinner("Rendering pages..."): 
            page_imgs = render_pdf_pages(pdf_bytes, dpi=140, max_pages=12) 
        st.session_state.page_images = page_imgs 
        st.caption(f"Preview (showing {len(page_imgs)} pages)") 
        cols = st.columns(4) 
        for i, (idx, im) in enumerate(page_imgs): 
            cols[i % 4].image(im, caption=f"Page {idx+1}", use_column_width=True) 
    if st.button(t["start_ocr"], type="primary", use_container_width=True): 
        def parse_range(s: str, total: int) -> List[int]: 
            pages = set() 
            for part in s.replace("ï¼Œ", ",").split(","): 
                if "-" in part: 
                    a, b = map(int, part.split("-")) 
                    pages.update(range(max(0, a-1), min(total, b))) 
                else: 
                    p = int(part) - 1 
                    if 0 <= p < total: 
                        pages.add(p) 
            return sorted(list(pages)) 
        selected = parse_range(page_range_input, len(page_imgs)) 
        if selected: 
            with st.spinner("Processing OCR..."): 
                if ocr_mode.startswith("Python"): 
                    text = extract_text_python(pdf_bytes, selected, ocr_lang) 
                else: 
                    text = extract_text_llm( 
                        [page_imgs[i][1] for i in selected], 
                        llm_ocr_model, 
                        router 
                    ) 
            st.session_state.ocr_text = text 
            st.balloons() 
            st.success("âœ… OCR Complete!") 
    st.markdown('</div>', unsafe_allow_html=True) 
# Tab 2: Preview & Edit
with tab2: 
    st.markdown('<div class="wow-card">', unsafe_allow_html=True) 
    st.subheader(f"{theme_icon} Document Text") 
    st.session_state.ocr_text = st.text_area( 
        "Edit OCR output", 
        value=st.session_state.ocr_text, 
        height=500, 
        label_visibility="collapsed" 
    ) 
    with st.expander("ğŸ” Keyword Highlighter"): 
        keywords = st.text_input("Keywords (comma-separated)", value="è—¥å“,é©æ‡‰ç—‡,ä¸è‰¯åæ‡‰") 
        if st.button("Highlight"): 
            out = st.session_state.ocr_text 
            for kw in keywords.split(","): 
                kw = kw.strip() 
                if kw: 
                    out = out.replace(kw, f"**:blue[{kw}]**") 
            st.markdown(out) 
    st.markdown('</div>', unsafe_allow_html=True) 
# Tab 3: Agent Config
with tab3: 
    st.markdown('<div class="wow-card">', unsafe_allow_html=True) 
    st.subheader(f"{theme_icon} Agent Configuration") 
    st.session_state.selected_agent_count = st.slider( 
        "Number of agents to use", 
        1, 
        len(st.session_state.agents_config), 
        min(5, len(st.session_state.agents_config)) 
    ) 
    global_prompt = st.text_area( 
        "Global System Prompt", 
        height=150, 
        value="""ä½ æ˜¯FDAæ–‡ä»¶åˆ†æå°ˆå®¶ï¼Œè«‹éµå¾ªï¼š1) ä¿æŒè³‡è¨Šæº–ç¢ºæ€§ï¼Œå¼•ç”¨åŸæ–‡æ™‚å¿…é ˆç²¾ç¢º2) çµæ§‹åŒ–è¼¸å‡ºï¼ˆè¡¨æ ¼ã€JSONã€æ¸…å–®ï¼‰3) æ¨™è¨»ä¸ç¢ºå®šé …ç›®ä¸¦èªªæ˜ç†ç”±4) è­˜åˆ¥æ½›åœ¨é¢¨éšªèˆ‡éœ€æ³¨æ„äº‹é …""" 
    ) 
    st.markdown("---") 
    for i in range(st.session_state.selected_agent_count): 
        agent = st.session_state.agents_config[i] 
        with st.expander(f"### Agent {i+1}: {agent.get('name', 'Unnamed')}", expanded=(i==0)): 
            st.markdown('<div class="agent-step">', unsafe_allow_html=True) 
            col1, col2 = st.columns([2, 1]) 
            with col1: 
                agent["system_prompt"] = st.text_area( 
                    "System Prompt", 
                    value=agent.get("system_prompt", ""), 
                    height=150, 
                    key=f"sys_{i}" 
                ) 
            with col2: 
                agent["model"] = st.selectbox( 
                    "Model", 
                    ["gpt-4o-mini", "gpt-5-nano", "gemini-2.5-flash", "gemini-2.5-flash-lite", "grok-3-mini"], 
                    index=0, 
                    key=f"model_{i}" 
                ) 
                agent["temperature"] = st.slider("Temp", 0.0, 2.0, float(agent.get("temperature", 0.3)), 0.1, key=f"temp_{i}") 
                agent["max_tokens"] = st.number_input("Max tokens", 64, 8192, int(agent.get("max_tokens", 1000)), 64, key=f"max_{i}") 
            st.markdown('</div>', unsafe_allow_html=True) 
    st.markdown('</div>', unsafe_allow_html=True) 
# Tab 4: Execute
with tab4: 
    st.markdown('<div class="wow-card">', unsafe_allow_html=True) 
    st.subheader(f"{theme_icon} Execute Agent Pipeline") 
    if not st.session_state.ocr_text.strip(): 
        st.warning("âš ï¸ Please complete OCR first (Tab 1)") 
    else: 
        # Initialize outputs if needed 
        if len(st.session_state.agent_outputs) < len(st.session_state.agents_config): 
            st.session_state.agent_outputs = [ 
                {"input": "", "output": "", "time": 0.0, "tokens": 0, "provider": "", "model": ""} 
                for _ in st.session_state.agents_config 
            ] 
        # Reset first agent input 
        if st.button("ğŸ”„ Reset Agent 1 Input to OCR Text"): 
            st.session_state.agent_outputs[0]["input"] = st.session_state.ocr_text 
            st.success("âœ… Reset!") 
        st.markdown("---") 
        # Agent pipeline 
        for i in range(st.session_state.selected_agent_count): 
            agent = st.session_state.agents_config[i] 
            st.markdown(f'<div class="agent-step">', unsafe_allow_html=True) 
            st.markdown(f"#### ğŸ¤– Agent {i+1}: {agent.get('name', '')}") 
            st.caption(agent.get('description', '')) 
            with st.expander("ğŸ“¥ Input (editable)", expanded=(i==0)): 
                default_input = st.session_state.ocr_text if i == 0 and not st.session_state.agent_outputs[i]["input"] else st.session_state.agent_outputs[i]["input"] 
                st.session_state.agent_outputs[i]["input"] = st.text_area( 
                    f"Agent {i+1} Input", 
                    value=default_input, 
                    height=200, 
                    key=f"in_{i}", 
                    label_visibility="collapsed" 
                ) 
            col_run, col_pass = st.columns([1, 2]) 
            with col_run: 
                if st.button(f"â–¶ï¸ Execute Agent {i+1}", key=f"run_{i}", type="primary"): 
                    with st.spinner(f"Agent {i+1} processing..."): 
                        t0 = time.time() 
                        messages = [ 
                            {"role": "system", "content": global_prompt}, 
                            {"role": "system", "content": agent.get("system_prompt", "")}, 
                            {"role": "user", "content": f"{agent.get('user_prompt', '')}\n\n{st.session_state.agent_outputs[i]['input']}"} 
                        ] 
                        params = { 
                            "temperature": float(agent.get("temperature", 0.3)), 
                            "top_p": float(agent.get("top_p", 0.95)), 
                            "max_tokens": int(agent.get("max_tokens", 1000)) 
                        } 
                        try: 
                            output, usage, provider = router.generate_text( 
                                agent.get("model", "gpt-4o-mini"), 
                                messages, 
                                params 
                            ) 
                            elapsed = time.time() - t0 
                            st.session_state.agent_outputs[i]["output"] = output 
                            st.session_state.agent_outputs[i]["time"] = elapsed 
                            st.session_state.agent_outputs[i]["tokens"] = usage.get("total_tokens", 0) 
                            st.session_state.agent_outputs[i]["provider"] = provider 
                            st.session_state.agent_outputs[i]["model"] = agent.get("model", "") 
                            st.session_state.run_metrics.append({ 
                                "agent": agent.get("name", ""), 
                                "latency": elapsed, 
                                "tokens": usage.get("total_tokens", 0), 
                                "provider": provider 
                            }) 
                            st.success(f"âœ… Completed in {elapsed:.2f}s | {usage.get('total_tokens', 0)} tokens") 
                            st.balloons() 
                        except Exception as e: 
                            st.error(f"âŒ Error: {str(e)}") 
            with col_pass: 
                if i < st.session_state.selected_agent_count - 1: 
                    if st.button(f"â¡ï¸ Pass to Agent {i+2}", key=f"pass_{i}"): 
                        st.session_state.agent_outputs[i+1]["input"] = st.session_state.agent_outputs[i]["output"] 
                        st.success(f"âœ… Passed to Agent {i+2}") 
                        st.rerun() 
            # Show output 
            st.markdown("##### ğŸ“¤ Output") 
            output_text = st.session_state.agent_outputs[i]["output"] 
            if output_text: 
                # Metrics 
                col_m1, col_m2, col_m3 = st.columns(3) 
                with col_m1: 
                    st.markdown(f'<div class="metric-card"><div class="metric-value">{st.session_state.agent_outputs[i]["time"]:.2f}s</div><div class="metric-label">Latency</div></div>', unsafe_allow_html=True) 
                with col_m2: 
                    st.markdown(f'<div class="metric-card"><div class="metric-value">{st.session_state.agent_outputs[i]["tokens"]}</div><div class="metric-label">Tokens</div></div>', unsafe_allow_html=True) 
                with col_m3: 
                    st.markdown(f'<div class="metric-card"><div class="metric-value">{st.session_state.agent_outputs[i]["provider"]}</div><div class="metric-label">Provider</div></div>', unsafe_allow_html=True) 
                st.text_area( 
                    f"Agent {i+1} Output", 
                    value=output_text, 
                    height=300, 
                    key=f"out_{i}", 
                    label_visibility="collapsed" 
                ) 
            st.markdown('</div>', unsafe_allow_html=True) 
            st.markdown("---") 
        # Export options 
        st.markdown("### ğŸ’¾ Export Results") 
        col_j, col_m, col_r = st.columns(3) 
        with col_j: 
            if st.button("ğŸ“¥ Download JSON", use_container_width=True): 
                import json 
                payload = { 
                    "timestamp": datetime.now().isoformat(), 
                    "theme": st.session_state.theme, 
                    "ocr_text": st.session_state.ocr_text, 
                    "agents": st.session_state.agents_config[:st.session_state.selected_agent_count], 
                    "outputs": st.session_state.agent_outputs[:st.session_state.selected_agent_count] 
                } 
                st.download_button( 
                    "Download JSON", 
                    data=json.dumps(payload, ensure_ascii=False, indent=2), 
                    file_name=f"fda_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json", 
                    mime="application/json", 
                    use_container_width=True 
                ) 
        with col_m: 
            if st.button("ğŸ“„ Download Markdown Report", use_container_width=True): 
                report = f"# FDA Document Analysis Report\n\n" 
                report += f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n" 
                report += f"**Theme:** {st.session_state.theme}\n\n" 
                report += f"## OCR Text\n\n{st.session_state.ocr_text}\n\n" 
                report += "---\n\n" 
                for i in range(st.session_state.selected_agent_count): 
                    agent = st.session_state.agents_config[i] 
                    report += f"## Agent {i+1}: {agent.get('name', '')}\n\n" 
                    report += f"**Description:** {agent.get('description', '')}\n\n" 
                    report += f"**Model:** {st.session_state.agent_outputs[i]['model']}\n\n" 
                    report += f"**Provider:** {st.session_state.agent_outputs[i]['provider']}\n\n" 
                    report += f"**Processing Time:** {st.session_state.agent_outputs[i]['time']:.2f}s\n\n" 
                    report += f"### Output\n\n{st.session_state.agent_outputs[i]['output']}\n\n" 
                    report += "---\n\n" 
                st.download_button( 
                    "Download Markdown", 
                    data=report, 
                    file_name=f"fda_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md", 
                    mime="text/markdown", 
                    use_container_width=True 
                ) 
        with col_r: 
            restore_file = st.file_uploader("ğŸ“¤ Restore Session JSON", type=["json"], key="restore") 
            if restore_file: 
                import json 
                data = json.loads(restore_file.read()) 
                st.session_state.ocr_text = data.get("ocr_text", "") 
                st.session_state.agents_config = data.get("agents", []) 
                st.session_state.agent_outputs = data.get("outputs", []) 
                st.session_state.selected_agent_count = len(st.session_state.agents_config) 
                st.success("âœ… Session restored!") 
                st.rerun() 
    st.markdown('</div>', unsafe_allow_html=True) 
# Tab 5: Dashboard
with tab5: 
    st.markdown('<div class="wow-card">', unsafe_allow_html=True) 
    st.subheader(f"{theme_icon} Analytics Dashboard") 
    if not st.session_state.run_metrics: 
        st.info("ğŸ“Š No data yet. Execute agents in Tab 4 to see analytics.") 
    else: 
        df = pd.DataFrame(st.session_state.run_metrics) 
        # Summary metrics 
        col1, col2, col3, col4 = st.columns(4) 
        with col1: 
            total_time = df['latency'].sum() 
            st.markdown(f'<div class="metric-card"><div class="metric-value">{total_time:.2f}s</div><div class="metric-label">Total Time</div></div>', unsafe_allow_html=True) 
        with col2: 
            total_tokens = df['tokens'].sum() 
            st.markdown(f'<div class="metric-card"><div class="metric-value">{total_tokens:,}</div><div class="metric-label">Total Tokens</div></div>', unsafe_allow_html=True) 
        with col3: 
            avg_latency = df['latency'].mean() 
            st.markdown(f'<div class="metric-card"><div class="metric-value">{avg_latency:.2f}s</div><div class="metric-label">Avg Latency</div></div>', unsafe_allow_html=True) 
        with col4: 
            agents_run = len(df) 
            st.markdown(f'<div class="metric-card"><div class="metric-value">{agents_run}</div><div class="metric-label">Agents Run</div></div>', unsafe_allow_html=True) 
        st.markdown("---") 
        # Charts 
        col_c1, col_c2 = st.columns(2) 
        with col_c1: 
            fig1 = px.bar( 
                df, 
                x="agent", 
                y="latency", 
                color="provider", 
                title="Agent Latency (seconds)", 
                color_discrete_map={"OpenAI": "#10a37f", "Gemini": "#4285f4", "Grok": "#ff6b6b"} 
            ) 
            fig1.update_layout( 
                plot_bgcolor='rgba(0,0,0,0)', 
                paper_bgcolor='rgba(0,0,0,0)', 
                font=dict(color=FLOWER_THEMES[st.session_state.theme]["accent"]) 
            ) 
            st.plotly_chart(fig1, use_container_width=True) 
        with col_c2: 
            fig2 = px.bar( 
                df, 
                x="agent", 
                y="tokens", 
                color="provider", 
                title="Token Usage by Agent", 
                color_discrete_map={"OpenAI": "#10a37f", "Gemini": "#4285f4", "Grok": "#ff6b6b"} 
            ) 
            fig2.update_layout( 
                plot_bgcolor='rgba(0,0,0,0)', 
                paper_bgcolor='rgba(0,0,0,0)', 
                font=dict(color=FLOWER_THEMES[st.session_state.theme]["accent"]) 
            ) 
            st.plotly_chart(fig2, use_container_width=True) 
        # Provider distribution 
        st.markdown("### Provider Distribution") 
        provider_counts = df['provider'].value_counts() 
        fig3 = px.pie( 
            values=provider_counts.values, 
            names=provider_counts.index, 
            title="API Calls by Provider", 
            color_discrete_map={"OpenAI": "#10a37f", "Gemini": "#4285f4", "Grok": "#ff6b6b"} 
        ) 
        fig3.update_layout( 
            plot_bgcolor='rgba(0,0,0,0)', 
            paper_bgcolor='rgba(0,0,0,0)', 
            font=dict(color=FLOWER_THEMES[st.session_state.theme]["accent"]) 
        ) 
        st.plotly_chart(fig3, use_container_width=True) 
        # Pipeline flow visualization 
        st.markdown("### Pipeline Flow") 
        try: 
            import graphviz 
            dot = graphviz.Digraph() 
            dot.attr(bgcolor='transparent') 
            dot.attr('node', shape='box', style='filled,rounded', fillcolor=FLOWER_THEMES[st.session_state.theme]["primary"]+'40', color=FLOWER_THEMES[st.session_state.theme]["accent"]) 
            for i, rec in enumerate(df.to_dict('records')): 
                label = f"{i+1}. {rec['agent']}\\n{rec['provider']}\\n{rec['latency']:.2f}s | {rec['tokens']} tok" 
                dot.node(f"a{i}", label) 
                if i > 0: 
                    dot.edge(f"a{i-1}", f"a{i}", color=FLOWER_THEMES[st.session_state.theme]["accent"]) 
            st.graphviz_chart(dot) 
        except Exception as e: 
            st.info(f"Graphviz visualization unavailable: {str(e)}") 
        # Detailed table 
        st.markdown("### Detailed Metrics") 
        st.dataframe( 
            df[['agent', 'provider', 'latency', 'tokens']].style.format({ 
                'latency': '{:.3f}s', 
                'tokens': '{:,}' 
            }), 
            use_container_width=True 
        ) 
    st.markdown('</div>', unsafe_allow_html=True) 
# Tab 6: Review Notes
with tab6:
    st.markdown('<div class="wow-card">', unsafe_allow_html=True)
    st.subheader(f"{theme_icon} å¯©æŸ¥ç­†è¨˜")
    st.info("åœ¨é€™è£¡ç·¨è¼¯æ‚¨çš„å¯©æŸ¥ç­†è¨˜ã€‚æ”¯æ´ Markdown å’Œ HTML é¡è‰²æ¨™ç±¤ï¼Œä¾‹å¦‚ <span style='color:blue'>è—è‰²æ–‡å­—</span>ã€‚ç­†è¨˜æœƒè‡ªå‹•å„²å­˜æ–¼æœƒè©±ä¸­ã€‚")
    st.session_state.review_notes = st.text_area(
        "ç·¨è¼¯ç­†è¨˜",
        value=st.session_state.review_notes,
        height=500,
        label_visibility="collapsed"
    )
    st.markdown("### é è¦½ç­†è¨˜")
    st.markdown(st.session_state.review_notes, unsafe_allow_html=True)
    if st.button("ç”¢ç”Ÿå¾ŒçºŒå•é¡Œå»ºè­°"):
        with st.spinner("ç”¢ç”Ÿä¸­..."):
            messages = [
                {"role": "system", "content": "ä½ æ˜¯å¯©æŸ¥å°ˆå®¶ï¼Œè«‹æ ¹æ“šæä¾›çš„ç­†è¨˜ç”Ÿæˆ 3-5 å€‹å¾ŒçºŒå•é¡Œï¼Œä»¥ Markdown æ¸…å–®æ ¼å¼è¼¸å‡ºã€‚"},
                {"role": "user", "content": st.session_state.review_notes}
            ]
            params = {"temperature": 0.5, "max_tokens": 500}
            output, _, _ = router.generate_text("gpt-4o-mini", messages, params)
            st.session_state.review_notes += f"\n\n## å¾ŒçºŒå•é¡Œå»ºè­°ï¼ˆè‡ªå‹•ç”Ÿæˆï¼‰\n{output}"
        st.success("âœ… å·²æ–°å¢å¾ŒçºŒå•é¡Œè‡³ç­†è¨˜æœ«å°¾ï¼")
        st.rerun()
    st.markdown('</div>', unsafe_allow_html=True)

# =================== NEW TAB: DOC COMPARE (Models) ===================
# Tab 7: Advanced
#tab_compare = st.tabs([
#    t["upload_tab"], t["preview_tab"], t["config_tab"], t["execute_tab"], t["dashboard_tab"], t["notes_tab"], "ğŸ” Doc Compare"
#])[-1]
#with tab_compare:
with tab7: 
    st.markdown('<div class="wow-card">', unsafe_allow_html=True)
    st.subheader(f"{theme_icon} {('Compare Agent/Models' if st.session_state.language == 'en' else 'æ¨¡å‹/ä»£ç†äººæ¯”å°')}")

    # 1. Paste or upload doc (text, markdown, json)
    doc_source = st.radio("è³‡æ–™ä¾†æº", ["è²¼ä¸Šæ–‡å­—", "ä¸Šå‚³æª”æ¡ˆ"], horizontal=True)
    doc_text = ""
    uploaded_doc = None

    if doc_source == "è²¼ä¸Šæ–‡å­—":
        doc_text = st.text_area("æ–‡ä»¶å…§å®¹", height=400, key="cmp_doc_text")
    else:
        uploaded_doc = st.file_uploader("ä¸Šå‚³Text/Markdown/JSON", type=["txt", "md", "json"], key="cmp_doc_upload")
        if uploaded_doc:
            content = uploaded_doc.read().decode("utf-8")
            doc_text = st.text_area("æ–‡ä»¶å…§å®¹", value=content, height=400, key="cmp_doc_text_fromupload")

    st.markdown("---")

    # 2. User selects: agent + 2 models + editable prompt for each
    st.markdown("#### é¸æ“‡ä»£ç†äººèˆ‡æ¨¡å‹")

    # Agent selector
    agent_names = [a.get("name", f"Agent {i+1}") for i, a in enumerate(st.session_state.agents_config)]
    agent_idx = st.selectbox("ä»£ç†äºº", agent_names, index=0)
    agent = st.session_state.agents_config[agent_names.index(agent_idx)]

    # Model selectors (can pick the same or different)
    models_available = ["gpt-4o-mini", "gpt-5-nano", "gpt-4.1-mini",
                        "gemini-2.5-flash", "gemini-2.5-flash-lite",
                        "grok-4-fast-reasoning", "grok-3-mini"]
    colm1, colm2 = st.columns(2)
    with colm1:
        model1 = st.selectbox("æ¨¡å‹ 1", models_available, key="cmp_model1")
    with colm2:
        model2 = st.selectbox("æ¨¡å‹ 2", models_available, index=1, key="cmp_model2")

    # Prompt editors
    colp1, colp2 = st.columns(2)
    with colp1:
        prompt1 = st.text_area("æ¨¡å‹ 1çš„ Prompt", value=agent.get("user_prompt", ""), height=100, key="cmp_prompt1")
    with colp2:
        prompt2 = st.text_area("æ¨¡å‹ 2çš„ Prompt", value=agent.get("user_prompt", ""), height=100, key="cmp_prompt2")

    # Optional: system prompt controls
    sys_prompt = agent.get("system_prompt", "")
    st.markdown("**[å¯é¸] System Promptï¼š**")
    sys_prompt = st.text_area("System Prompt", value=sys_prompt, height=80, key="cmp_sys_prompt")

    # ======== EXECUTE AND COMPARE ========
    run_btn = st.button("ğŸš€ æ¯”è¼ƒæ¨¡å‹", type="primary", use_container_width=True)
    result1, result2 = "", ""
    time1 = time2 = 0.0

    if run_btn and doc_text.strip():
        params = {"temperature": agent.get("temperature", 0.3), "top_p": agent.get("top_p", 0.95), "max_tokens": agent.get("max_tokens", 1000)}
        with st.spinner("åŸ·è¡Œæ¨¡å‹..."):
            # Compose message for both models
            messages1 = [{"role": "system", "content": sys_prompt}, {"role": "user", "content": f"{prompt1}\n\n{doc_text}"}]
            messages2 = [{"role": "system", "content": sys_prompt}, {"role": "user", "content": f"{prompt2}\n\n{doc_text}"}]
            # Run Model 1
            t0 = time.time()
            try:
                out1, usage1, prov1 = router.generate_text(model1, messages1, params)
                time1 = time.time() - t0
            except Exception as e:
                out1, time1, prov1 = f"[âŒ Error: {str(e)}]", 0.0, ""
                usage1 = {}
            # Run Model 2
            t0 = time.time()
            try:
                out2, usage2, prov2 = router.generate_text(model2, messages2, params)
                time2 = time.time() - t0
            except Exception as e:
                out2, time2, prov2 = f"[âŒ Error: {str(e)}]", 0.0, ""
                usage2 = {}

        result1, result2 = out1, out2

        # Show comparison SIDE BY SIDE:
        c1, c2 = st.columns(2)
        with c1:
            st.markdown(f"### ğŸ¥‡ {model1} ({prov1})")
            st.markdown(f"*è€—æ™‚*: {time1:.2f}s, *Tokens*: {usage1.get('total_tokens', '-')}")
            st.text_area("Output 1", value=result1, height=400, key="cmp_out1")
        with c2:
            st.markdown(f"### ğŸ¥ˆ {model2} ({prov2})")
            st.markdown(f"*è€—æ™‚*: {time2:.2f}s, *Tokens*: {usage2.get('total_tokens', '-')}")
            st.text_area("Output 2", value=result2, height=400, key="cmp_out2")

        # Diff or feedback region
        with st.expander("ğŸ” å·®ç•°/å¿ƒå¾—æ¯”è¼ƒ"):
            try:
                import difflib
                html_diff = difflib.HtmlDiff().make_table(result1.splitlines(), result2.splitlines(), model1, model2)
                st.markdown(html_diff, unsafe_allow_html=True)
            except Exception as e:
                st.info("Diff unavailable. Install python stdlib difflib for side-by-side diff.")

    st.markdown("""
    <ul>
     <li>ä½ å¯ä»¥æ›´æ”¹ <b>æ–‡ä»¶å…§å®¹</b> å†æ¬¡æ¯”å°</li>
     <li>å¯éš¨æ™‚åˆ‡æ›ä»£ç†äººæˆ–æ¨¡å‹çµ„åˆï¼Œä¿®æ”¹ Prompt å¾Œé‡æ–°æ¯”è¼ƒ</li>
     <li>å±•é–‹å·®ç•°æŸ¥çœ‹å·®ç•°è¡Œ/é€è¡Œå°æ¯”ï¼Œæˆ–å°‡å…¶ä¸­ä¸€å´çµæœè¤‡è£½åˆ°ä¸» pipeline</li>
    </ul>
    """, unsafe_allow_html=True)
    st.markdown('</div>', unsafe_allow_html=True)
# ==================== FOOTER ====================
st.markdown("---")
st.markdown(f"""<div style="text-align: center; padding: 2rem; opacity: 0.7;"> 
    <p>{theme_icon} <strong>TFDA Agentic AI Assistance Review System</strong></p> 
    <p>Powered by OpenAI, Google Gemini & xAI Grok â€¢ Built with Streamlit</p> 
    <p style="font-size: 0.8rem;">Â© 2024 â€¢ Theme: {st.session_state.theme}</p></div>""", unsafe_allow_html=True)
